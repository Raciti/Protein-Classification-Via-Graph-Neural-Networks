{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e010fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc6bfef",
   "metadata": {},
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "'meta.yaml' cannot be found under ../../Dati/Dataset_Binario/.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCSVDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../Dati/Dataset_Binario/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dataset\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/dgl/data/csv_dataset.py:80\u001b[0m, in \u001b[0;36mCSVDataset.__init__\u001b[0;34m(self, data_path, force_reload, verbose, ndata_parser, edata_parser, gdata_parser, transform)\u001b[0m\n\u001b[1;32m     78\u001b[0m meta_yaml_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, CSVDataset\u001b[38;5;241m.\u001b[39mMETA_YAML_NAME)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(meta_yaml_path):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DGLError(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot be found under \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(CSVDataset\u001b[38;5;241m.\u001b[39mMETA_YAML_NAME, data_path))\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_yaml \u001b[38;5;241m=\u001b[39m load_yaml_with_sanity_check(meta_yaml_path)\n\u001b[1;32m     83\u001b[0m ds_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_yaml\u001b[38;5;241m.\u001b[39mdataset_name\n",
      "\u001b[0;31mDGLError\u001b[0m: 'meta.yaml' cannot be found under ../../Dati/Dataset_Binario/."
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CSVDataset('../../Dati/Dataset_Binario/')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9714a20f",
   "metadata": {},
   "source": [
    "#PROVAAAA\n",
    "\n",
    "g, inf = dataset[0]\n",
    "print(g)\n",
    "print(inf)\n",
    "print(g.nodes())\n",
    "print(g.ndata[\"label\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69c559c1",
   "metadata": {},
   "source": [
    "for g,i in dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19149971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numer_class = []\n",
    "a = 0\n",
    "b = 0 \n",
    "c = 0\n",
    "d = 0\n",
    "e = 0\n",
    "f = 0\n",
    "gg = 0\n",
    "h = 0\n",
    "for g, i in dataset:\n",
    "    if i[\"label\"].item() not in numer_class:\n",
    "        numer_class.append(i[\"label\"].item())\n",
    "    if i[\"label\"].item() == 0:\n",
    "        a += 1\n",
    "    elif i[\"label\"].item() == 1:\n",
    "        b += 1\n",
    "    elif i[\"label\"].item() == 2:\n",
    "        c += 1\n",
    "    elif i[\"label\"].item() == 3:\n",
    "        d += 1\n",
    "    elif i[\"label\"].item() == 4:\n",
    "        e += 1\n",
    "    elif i[\"label\"].item() == 5:\n",
    "        f += 1\n",
    "    elif i[\"label\"].item() == 6:\n",
    "        gg += 1\n",
    "    elif i[\"label\"].item() == 7:\n",
    "        h += 1\n",
    "\n",
    "print(a, b, c, d, e, f, gg, h, (a+b+c+d+e+f+gg+h))\n",
    "print(len(numer_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_nfeats = 2\n",
    "gclasses = len(numer_class)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a69260c",
   "metadata": {},
   "source": [
    "if gclasses == 2:\n",
    "    size_batch = 4\n",
    "elif gclasses >= 2:\n",
    "    size_batch = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_batch = len(numer_class)*2\n",
    "size_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adbc35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_examples = len(dataset)\n",
    "num_train = int(num_examples * 0.8)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train))\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n",
    "\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size= size_batch, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=test_sampler, batch_size= size_batch, drop_last=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "008ee009",
   "metadata": {},
   "source": [
    "print(num_examples)\n",
    "print(num_train)\n",
    "print(train_sampler)\n",
    "print(test_sampler)\n",
    "print(train_dataloader)\n",
    "print(test_dataloader)\n",
    "print(type(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc382d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "it = iter(train_dataloader)\n",
    "batch = next(it)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8963483",
   "metadata": {},
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d1f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_graph, labels = batch\n",
    "print('Number of nodes for each graph element in the batch:', batched_graph.batch_num_nodes())\n",
    "print('Number of edges for each graph element in the batch:', batched_graph.batch_num_edges())\n",
    "\n",
    "# Recover the original graph elements from the minibatch\n",
    "graphs = dgl.unbatch(batched_graph)\n",
    "print('The original graphs in the minibatch:')\n",
    "print(graphs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "820e0d7f",
   "metadata": {},
   "source": [
    "l = labels['label']\n",
    "print(l)\n",
    "print(type(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        return dgl.mean_nodes(g, 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gclasses == 2:\n",
    "    num_of_epoches = 200\n",
    "elif gclasses >= 2:\n",
    "    num_of_epoches = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987cc2f",
   "metadata": {},
   "source": [
    "## Analisi risultati\n",
    "Con il dataset Binario l'accuratezza si aggira attorno al 0.9.\n",
    "Con il dataset Multiplo il quale momentaneamente contiene 4 classi di grafi l'accuratezza si aggira attorno al 0.54."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513ec2d",
   "metadata": {},
   "source": [
    "## PROVE\n",
    "\n",
    "\n",
    "Numero di nodi 16\n",
    "\n",
    "batch = 10 epoche 1000 con soglia 400epoche e loss < 0.4 -> ***ACCURATEZZA OTTENUTA*** 0,61\n",
    "\n",
    "batch = 8 epoche 5000  -> ***ACCURATEZZA OTTENUTA*** 0,61 con ultima loss -> 0.7\n",
    "\n",
    "batch = 8 epoche 1000  -> ***ACCURATEZZA OTTENUTA*** 0,59 con ultima loss -> 0.87\n",
    "\n",
    "batch = 8 epoche 1000  -> ***ACCURATEZZA OTTENUTA*** 0,63 con ultima loss -> 1.61\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Numero di nodi 32\n",
    "\n",
    "batch = 8 epoche 1000 ->  ***ACCURATEZZA OTTENUTA*** 0,62 con ultima loss ->  1.09\n",
    "\n",
    "\n",
    "Numero di nodi 12\n",
    "\n",
    "batch = 8 epoche 1000 ->  ***ACCURATEZZA OTTENUTA*** 0,62 con ultima loss ->   0.75\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Numero di nodi 12 classificazione di sei classi \n",
    "\n",
    "batch = 12 epoche 1000 -> ***ACCURATEZZA OTTENUTA*** 0,42 con ultima loss ->  0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beace8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from os.path import join\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class AverageValueMeter():\n",
    "    \"\"\" Compute the average loss between batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "        \n",
    "    def add(self, value, num):\n",
    "        self.sum += value * num\n",
    "        self.num += num\n",
    "        \n",
    "    def value(self):\n",
    "        try: \n",
    "            return self.sum / self.num\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "\n",
    "def train_classifier(model: nn.Module, \n",
    "                     train_loader, \n",
    "                     test_loader, \n",
    "                     exp_name=\"experiment\", \n",
    "                     lr=0.001, \n",
    "                     epochs=10, \n",
    "                     momentum=0.9, \n",
    "                     logdir=\"logs\"):\n",
    "    \"\"\" Train the model.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_meter = AverageValueMeter()\n",
    "    accr_meter = AverageValueMeter()\n",
    "    writer = SummaryWriter(join(logdir, exp_name))\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    loader = {\n",
    "        'train': train_loader, \n",
    "        'test':  test_loader\n",
    "    }\n",
    "    \n",
    "    global_step = 0\n",
    "    for e in tqdm(range(epochs)): \n",
    "        for mode in ['train', 'test']:\n",
    "            loss_meter.reset()\n",
    "            accr_meter.reset()\n",
    "            model.train() if mode == 'train' else model.eval()\n",
    "            with torch.set_grad_enabled(mode=='train'):\n",
    "                for i, batch in enumerate(loader[mode]):\n",
    "                    \n",
    "                    \n",
    "                    #batched_graph, batched_graph.ndata['label'].float()\n",
    "                    \n",
    "                    x = batch[0]#.to(device)\n",
    "                    y = batch[1]['label'].long()#.to(device)\n",
    "                    #print(y['label'])\n",
    "                    \n",
    "                    output = model(x, x.ndata['label'].float())\n",
    "                    \n",
    "                    n = size_batch # elements in batch\n",
    "                    global_step += n\n",
    "                    l = criterion(output, y)\n",
    "                    \n",
    "                    if mode == 'train':\n",
    "                        l.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                    acc = accuracy_score(\n",
    "                        y.to('cpu'), \n",
    "                        output.to('cpu').max(1)[1]\n",
    "                    )\n",
    "                    loss_meter.add(l.item(), n)\n",
    "                    accr_meter.add(acc, n)\n",
    "                    \n",
    "                    if mode == 'train':\n",
    "                        writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
    "                        writer.add_scalar('accuracy/train', accr_meter.value(), global_step=global_step)\n",
    "            \n",
    "            writer.add_scalar('loss/test', loss_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('accuracy/test', accr_meter.value(), global_step=global_step)\n",
    "        torch.save(model.state_dict(), '%s-%d.pth' % (exp_name, e+1))\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def test_classifier(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    predictions, labels = [], []\n",
    "    for batch in loader:\n",
    "        x = batch[0]#.to(device)\n",
    "        y = batch[1]['label'].long()#.to(device)\n",
    "        output = model(x, x.ndata['label'].float())\n",
    "        preds = output.to('cpu').max(1)[1].numpy()\n",
    "        labs = y.to('cpu').numpy()\n",
    "        predictions.extend(list(preds))\n",
    "        labels.extend(list(labs))\n",
    "    return np.array(predictions), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GCN(dim_nfeats, 12, gclasses)\n",
    "\n",
    "model2 = train_classifier(\n",
    "    model2,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    'gnn_experiment',\n",
    "    epochs = num_of_epoches\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbfca92b",
   "metadata": {},
   "source": [
    "cd Desktop/Università/Tesi/Python/Dgl/\n",
    "tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c330250",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificazione = test_classifier(model2, test_dataloader)\n",
    "print(classificazione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e512313",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(classificazione[0], classificazione[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
